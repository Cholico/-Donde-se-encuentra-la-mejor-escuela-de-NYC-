{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgwBwR4cfORwjDWcj9VAXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cholico/Proyectos-de-DataCamp-/blob/main/Depuraci%C3%B3n_de_un_flujo_de_trabajo_de_datos_de_ventas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZwbth5h9r0q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como ingeniero de datos, a menudo se enfrenta a desafíos inesperados en los flujos de trabajo. En este escenario, la función `load_and_check()`, a cargo de administrar los datos de ventas, encuentra problemas después de la última actualización. Desafortunadamente, su colega que generalmente maneja este código está actualmente de vacaciones, lo que lo deja a usted solo para solucionar los problemas.\n",
        "\n",
        "Su tarea es identificar y abordar los problemas en el flujo de datos de ventas **sin analizar cada línea de código**. La función `load_and_check()` carga el conjunto de datos `sales.csv` y realiza varias comprobaciones. Inicialmente, verifica la forma del conjunto de datos, asegurándose de que coincida con las expectativas. Posteriormente, se realizan comprobaciones de integridad para mantener la coherencia de los datos y marcar cualquier anomalía.\n",
        "\n",
        "El conjunto de datos `sales.csv` tiene varias columnas, que se centran en campos críticos como `Total`, `Cantidad`, `Precio unitario`, `Impuesto` y `Fecha`. Es fundamental que la columna \"Impuestos\" represente con precisión el 5 % del subtotal, calculado a partir del \"Precio unitario\" multiplicado por la \"Cantidad\".\n",
        "\n",
        "**Su objetivo es resolver los problemas de la canalización, con el objetivo de que el código devuelva 2 mensajes de éxito al finalizar.** Mientras tanto, intente mantener la estructura original tanto como sea posible. Cambie las columnas existentes solo si es necesario y asegúrese de que los datos sigan siendo precisos. Tenga en cuenta la actualización de las declaraciones if relevantes en los controles según sea necesario."
      ],
      "metadata": {
        "id": "bq2HPCJb95Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bFEbGzwJBMiN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_check():\n",
        "    # Step 1: Load the data and check if it has the expected shape\n",
        "    data = pd.read_csv('sales.csv')\n",
        "\n",
        "    if data.shape[1] != 17:\n",
        "        print(\"Please check that the data was loaded properly!\")\n",
        "    else:\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Step 2: Calculate statistical values and merge with the original data\n",
        "    grouped_data = data.groupby(['Date'])['Total'].agg(['mean', 'std'])\n",
        "    grouped_data['threshold'] = 3 * grouped_data['std']\n",
        "    grouped_data['max'] = grouped_data['mean'] + grouped_data.threshold\n",
        "    grouped_data['min'] = grouped_data[['mean', 'threshold']].apply(lambda row: max(0, row['mean'] - row['threshold']), axis=1)\n",
        "    data = pd.merge(data, grouped_data, on='Date', how='left')\n",
        "\n",
        "    # Condition_1 checks if 'Total' is within the acceptable range (min to max) for each date\n",
        "    data['Condition_1'] = (data['Total'] >= data['min']) & (data['Total'] <= data['max'])\n",
        "    data['Condition_1'].fillna(False, inplace=True)\n",
        "\n",
        "    # Condition_2 checks if the 'Tax' column is properly calculated as 5% of (Quantity * Unit price)\n",
        "    data['Condition_2'] = round(data['Quantity'] * data['Unit price'] * 0.05, 1) == round(data['Tax'], 1)\n",
        "\n",
        "\n",
        "    # Step 3: Check if all rows pass both Condition_1 and Condition_2\n",
        "    # Success indicates data integrity; failure suggests potential issues.\n",
        "    if (data['Condition_1'].sum() == data.shape[0]) and (data['Condition_2'].sum() == data.shape[0]):\n",
        "        print(\"Data integrity check was successful! All rows pass the integrity conditions.\")\n",
        "    else:\n",
        "        print(\"Something fishy is going on with the data! Integrity check failed for some rows!\")\n",
        "\n",
        "    return data\n",
        "\n",
        "processed_data = load_and_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a32wtYmV97Hj",
        "outputId": "6d22bbc1-fd52-46d4-ddff-3a37fb7eb77c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please check that the data was loaded properly!\n",
            "Something fishy is going on with the data! Integrity check failed for some rows!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_check():\n",
        "    # Step 1: Load the data and check if it has the expected shape\n",
        "    data = pd.read_csv('sales.csv')\n",
        "\n",
        "    # Issue 1 fixed: Correct number of expected columns\n",
        "    if data.shape[1] != 17:\n",
        "        print(\"Please check that the data was loaded properly!\")\n",
        "    else:\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "    # Step 2: Calculate statistical values and merge with the original data\n",
        "    grouped_data = data.groupby(['Date'])['Total'].agg(['mean', 'std'])\n",
        "    grouped_data['threshold'] = 3 * grouped_data['std']\n",
        "    grouped_data['max'] = grouped_data['mean'] + grouped_data.threshold\n",
        "    grouped_data['min'] = grouped_data[['mean', 'threshold']].apply(lambda row: max(0, row['mean'] - row['threshold']), axis=1)\n",
        "    data = pd.merge(data, grouped_data, on='Date', how='left')\n",
        "\n",
        "    # Issue 2 fixed:  Recalculating the 'Tax' column\n",
        "    data['Tax'] = (data['Quantity'] * data['Unit price']).astype(float) * 0.05  # Assuming tax is 5% of the subtotal\n",
        "\n",
        "    # Condition_1 checks if 'Total' is within the acceptable range (min to max) for each date\n",
        "    data['Condition_1'] = (data['Total'] >= data['min']) & (data['Total'] <= data['max'])\n",
        "    data['Condition_1'].fillna(False, inplace=True)\n",
        "\n",
        "    # Condition_2 checks if the 'Tax' column is properly calculated as 5% of (Quantity * Unit price)\n",
        "    data['Condition_2'] = round(data['Quantity'] * data['Unit price'] * 0.05, 1) == round(data['Tax'], 1)\n",
        "\n",
        "    # Step 3: Check if all rows pass both Condition_1 and Condition_2\n",
        "    # Success indicates data integrity; failure suggests potential issues.\n",
        "    if (data['Condition_1'].sum() == data.shape[0]) and (data['Condition_2'].sum() == data.shape[0]):\n",
        "        print(\"Data integrity check was successful! All rows pass the integrity conditions.\")\n",
        "    else:\n",
        "        print(\"Something fishy is going on with the data! Integrity check failed for some rows!\")\n",
        "\n",
        "    return data\n",
        "\n",
        "processed_data = load_and_check()"
      ],
      "metadata": {
        "id": "YuYcWcr8bjzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}